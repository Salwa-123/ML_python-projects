{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd7e41f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# import custom dataset classes\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XRaysTrainDataset  \n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XRaysTestDataset\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# import neccesary libraries for defining the optimizers\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os, pdb, sys, glob, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models \n",
    "\n",
    "# import custom dataset classes\n",
    "from datasets import XRaysTrainDataset  \n",
    "from datasets import XRaysTestDataset\n",
    "\n",
    "# import neccesary libraries for defining the optimizers\n",
    "import torch.optim as optim\n",
    "\n",
    "from trainer import fit\n",
    "import config\n",
    "\n",
    "def q(text = ''): # easy way to exiting the script. useful while debugging\n",
    "    print('> ', text)\n",
    "    sys.exit()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'\\ndevice: {device}')\n",
    "    \n",
    "parser = argparse.ArgumentParser(description='Following are the arguments that can be passed form the terminal itself ! Cool huh ? :D')\n",
    "parser.add_argument('--data_path', type = str, default = 'NIH Chest X-rays', help = 'This is the path of the training data')\n",
    "parser.add_argument('--bs', type = int, default = 128, help = 'batch size')\n",
    "parser.add_argument('--lr', type = float, default = 1e-5, help = 'Learning Rate for the optimizer')\n",
    "parser.add_argument('--stage', type = int, default = 1, help = 'Stage, it decides which layers of the Neural Net to train')\n",
    "parser.add_argument('--loss_func', type = str, default = 'FocalLoss', choices = {'BCE', 'FocalLoss'}, help = 'loss function')\n",
    "parser.add_argument('-r','--resume', action = 'store_true') # args.resume will return True if -r or --resume is used in the terminal\n",
    "parser.add_argument('--ckpt', type = str, help = 'Path of the ckeckpoint that you wnat to load')\n",
    "parser.add_argument('-t','--test', action = 'store_true')   # args.test   will return True if -t or --test   is used in the terminal\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.resume and args.test: # what if --test is not defiend at all ? test case hai ye ek\n",
    "    q('The flow of this code has been designed either to train the model or to test it.\\nPlease choose either --resume or --test')\n",
    "\n",
    "stage = args.stage\n",
    "if not args.resume:\n",
    "    print(f'\\nOverwriting stage to 1, as the model training is being done from scratch')\n",
    "    stage = 1\n",
    "    \n",
    "if args.test:\n",
    "    print('TESTING THE MODEL')\n",
    "else:\n",
    "    if args.resume:\n",
    "        print('RESUMING THE MODEL TRAINING')\n",
    "    else:\n",
    "        print('TRAINING THE MODEL FROM SCRATCH')\n",
    "\n",
    "script_start_time = time.time() # tells the total run time of this script\n",
    "\n",
    "# mention the path of the data\n",
    "data_dir = os.path.join('data',args.data_path) # Data_Entry_2017.csv should be present in the mentioned path\n",
    "\n",
    "# define a function to count the total number of trainable parameters\n",
    "def count_parameters(model): \n",
    "    num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return num_parameters/1e6 # in terms of millions\n",
    "\n",
    "# make the datasets\n",
    "XRayTrain_dataset = XRaysTrainDataset(data_dir, transform = config.transform)\n",
    "train_percentage = 0.8\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(XRayTrain_dataset, [int(len(XRayTrain_dataset)*train_percentage), len(XRayTrain_dataset)-int(len(XRayTrain_dataset)*train_percentage)])\n",
    "\n",
    "XRayTest_dataset = XRaysTestDataset(data_dir, transform = config.transform)\n",
    "\n",
    "print('\\n-----Initial Dataset Information-----')\n",
    "print('num images in train_dataset   : {}'.format(len(train_dataset)))\n",
    "print('num images in val_dataset     : {}'.format(len(val_dataset)))\n",
    "print('num images in XRayTest_dataset: {}'.format(len(XRayTest_dataset)))\n",
    "print('-------------------------------------')\n",
    "\n",
    "# make the dataloaders\n",
    "batch_size = args.bs # 128 by default\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size, shuffle = not True)\n",
    "test_loader = torch.utils.data.DataLoader(XRayTest_dataset, batch_size = batch_size, shuffle = not True)\n",
    "\n",
    "print('\\n-----Initial Batchloaders Information -----')\n",
    "print('num batches in train_loader: {}'.format(len(train_loader)))\n",
    "print('num batches in val_loader  : {}'.format(len(val_loader)))\n",
    "print('num batches in test_loader : {}'.format(len(test_loader)))\n",
    "print('-------------------------------------------')\n",
    "\n",
    "# sanity check\n",
    "if len(XRayTrain_dataset.all_classes) != 15: # 15 is the unique number of diseases in this dataset\n",
    "    q('\\nnumber of classes not equal to 15 !')\n",
    "\n",
    "a,b = train_dataset[0]\n",
    "print('\\nwe are working with \\nImages shape: {} and \\nTarget shape: {}'.format( a.shape, b.shape))\n",
    "\n",
    "# make models directory, where the models and the loss plots will be saved\n",
    "if not os.path.exists(config.models_dir):\n",
    "    os.mkdir(config.models_dir)\n",
    "\n",
    "# define the loss function\n",
    "if args.loss_func == 'FocalLoss': # by default\n",
    "    from losses import FocalLoss\n",
    "    loss_fn = FocalLoss(device = device, gamma = 2.).to(device)\n",
    "elif args.loss_func == 'BCE':\n",
    "    loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "# define the learning rate\n",
    "lr = args.lr\n",
    "\n",
    "if not args.test: # training\n",
    "\n",
    "    # initialize the model if not args.resume\n",
    "    if not args.resume:\n",
    "        print('\\ntraining from scratch')\n",
    "        # import pretrained model\n",
    "        model = models.resnet50(pretrained=True) # pretrained = False bydefault\n",
    "        # change the last linear layer\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, len(XRayTrain_dataset.all_classes)) # 15 output classes \n",
    "        model.to(device)\n",
    "        \n",
    "        print('----- STAGE 1 -----') # only training 'layer2', 'layer3', 'layer4' and 'fc'\n",
    "        for name, param in model.named_parameters(): # all requires_grad by default, are True initially\n",
    "            # print('{}: {}'.format(name, param.requires_grad)) # this shows True for all the parameters  \n",
    "            if ('layer2' in name) or ('layer3' in name) or ('layer4' in name) or ('fc' in name):\n",
    "                param.requires_grad = True \n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # since we are not resuming the training of the model\n",
    "        epochs_till_now = 0\n",
    "\n",
    "        # making empty lists to collect all the losses\n",
    "        losses_dict = {'epoch_train_loss': [], 'epoch_val_loss': [], 'total_train_loss_list': [], 'total_val_loss_list': []}\n",
    "\n",
    "    else:\n",
    "        if args.ckpt == None:\n",
    "            q('ERROR: Please select a valid checkpoint to resume from')\n",
    "            \n",
    "        print('\\nckpt loaded: {}'.format(args.ckpt))\n",
    "        ckpt = torch.load(os.path.join(config.models_dir, args.ckpt)) \n",
    "\n",
    "        # since we are resuming the training of the model\n",
    "        epochs_till_now = ckpt['epochs']\n",
    "        model = ckpt['model']\n",
    "        model.to(device)\n",
    "        \n",
    "        # loading previous loss lists to collect future losses\n",
    "        losses_dict = ckpt['losses_dict']\n",
    "\n",
    "    # printing some hyperparameters\n",
    "    print('\\n> loss_fn: {}'.format(loss_fn))\n",
    "    print('> epochs_till_now: {}'.format(epochs_till_now))\n",
    "    print('> batch_size: {}'.format(batch_size))\n",
    "    print('> stage: {}'.format(stage))\n",
    "    print('> lr: {}'.format(lr))\n",
    "\n",
    "else: # testing\n",
    "    if args.ckpt == None:\n",
    "        q('ERROR: Please select a checkpoint to load the testing model from')\n",
    "        \n",
    "    print('\\ncheckpoint loaded: {}'.format(args.ckpt))\n",
    "    ckpt = torch.load(os.path.join(config.models_dir, args.ckpt)) \n",
    "\n",
    "    # since we are resuming the training of the model\n",
    "    epochs_till_now = ckpt['epochs']\n",
    "    model = ckpt['model']\n",
    "    \n",
    "    # loading previous loss lists to collect future losses\n",
    "    losses_dict = ckpt['losses_dict']\n",
    "\n",
    "# make changes(freezing/unfreezing the model's layers) in the following, for training the model for different stages \n",
    "if (not args.test) and (args.resume):\n",
    "\n",
    "    if stage == 1:\n",
    "\n",
    "        print('\\n----- STAGE 1 -----') # only training 'layer2', 'layer3', 'layer4' and 'fc'\n",
    "        for name, param in model.named_parameters(): # all requires_grad by default, are True initially\n",
    "            # print('{}: {}'.format(name, param.requires_grad)) # this shows True for all the parameters  \n",
    "            if ('layer2' in name) or ('layer3' in name) or ('layer4' in name) or ('fc' in name):\n",
    "                param.requires_grad = True \n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    elif stage == 2:\n",
    "\n",
    "        print('\\n----- STAGE 2 -----') # only training 'layer3', 'layer4' and 'fc'\n",
    "        for name, param in model.named_parameters(): \n",
    "            # print('{}: {}'.format(name, param.requires_grad)) # this shows True for all the parameters  \n",
    "            if ('layer3' in name) or ('layer4' in name) or ('fc' in name):\n",
    "                param.requires_grad = True \n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    elif stage == 3:\n",
    "\n",
    "        print('\\n----- STAGE 3 -----') # only training  'layer4' and 'fc'\n",
    "        for name, param in model.named_parameters(): \n",
    "            # print('{}: {}'.format(name, param.requires_grad)) # this shows True for all the parameters  \n",
    "            if ('layer4' in name) or ('fc' in name):\n",
    "                param.requires_grad = True \n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    elif stage == 4:\n",
    "\n",
    "        print('\\n----- STAGE 4 -----') # only training 'fc'\n",
    "        for name, param in model.named_parameters(): \n",
    "            # print('{}: {}'.format(name, param.requires_grad)) # this shows True for all the parameters  \n",
    "            if ('fc' in name):\n",
    "                param.requires_grad = True \n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "if not args.test:\n",
    "    # checking the layers which are going to be trained (irrespective of args.resume)\n",
    "    trainable_layers = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            layer_name = str.split(name, '.')[0]\n",
    "            if layer_name not in trainable_layers: \n",
    "                trainable_layers.append(layer_name)\n",
    "    print('\\nfollowing are the trainable layers...')\n",
    "    print(trainable_layers)\n",
    "\n",
    "    print('\\nwe have {} Million trainable parameters here in the {} model'.format(count_parameters(model), model.__class__.__name__))\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = lr)\n",
    "\n",
    "# make changes in the parameters of the following 'fit' function\n",
    "fit(device, XRayTrain_dataset, train_loader, val_loader,    \n",
    "                                        test_loader, model, loss_fn, \n",
    "                                        optimizer, losses_dict,\n",
    "                                        epochs_till_now = epochs_till_now, epochs = 3,\n",
    "                                        log_interval = 25, save_interval = 1,\n",
    "                                        lr = lr, bs = batch_size, stage = stage,\n",
    "                                        test_only = args.test)\n",
    "\n",
    "script_time = time.time() - script_start_time\n",
    "m, s = divmod(script_time, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print('{} h {}m laga poore script me !'.format(int(h), int(m)))\n",
    "\n",
    "# ''' \n",
    "# This is how the model is trained...\n",
    "# ##### STAGE 1 ##### FocalLoss lr = 1e-5\n",
    "# training layers = layer2, layer3, layer4, fc \n",
    "# epochs = 2\n",
    "# ##### STAGE 2 ##### FocalLoss lr = 3e-4\n",
    "# training layers = layer3, layer4, fc \n",
    "# epochs = 5\n",
    "# ##### STAGE 3 ##### FocalLoss lr = 7e-4\n",
    "# training layers = layer4, fc \n",
    "# epochs = 4\n",
    "# ##### STAGE 4 ##### FocalLoss lr = 1e-3\n",
    "# training layers = fc \n",
    "# epochs = 3\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef96cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
